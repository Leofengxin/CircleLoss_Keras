{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "def build_session(gpu_fraction=0.2, allow_growth=True):\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction,\n",
    "                                allow_growth=allow_growth)\n",
    "    return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "sess = build_session()\n",
    "K.set_session(sess)\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# # convert class vectors to binary class matrices\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_dist(input_labels,inputs,flag_l2norm=True,):\n",
    "    # do softmax cross-entropy\n",
    "    lshape = K.tf.shape(input_labels)\n",
    "    #assert lshape.shape == 1\n",
    "    labels = K.tf.reshape(input_labels, [lshape[0], 1])\n",
    "\n",
    "    mask = K.tf.to_int32(K.tf.equal(labels, K.tf.transpose(labels)))\n",
    "    positive_mask = K.tf.matrix_band_part(mask, 0, -1) - tf.matrix_band_part(mask, 0, 0)\n",
    "    negative_mask = K.tf.matrix_band_part(1 - mask, 0, -1)\n",
    "#     print('gcont: flag_l2norm',flag_l2norm)\n",
    "    if flag_l2norm:\n",
    "        inputs = K.l2_normalize(inputs,axis=1)\n",
    "        \n",
    "    sim = K.tf.matmul(inputs, K.tf.transpose(inputs))\n",
    "    # 使其取值范围在0-1区间上，sn目标为0而sp目标为1\n",
    "#     sim = (sim + 1.) / 2.\n",
    "    pos_sim = tf.boolean_mask(sim, positive_mask)\n",
    "    neg_sim = tf.boolean_mask(sim, negative_mask)\n",
    "    \n",
    "    return pos_sim, neg_sim\n",
    "\n",
    "\n",
    "def circle_loss(input_labels,inputs,margin = 0.25, gamma=256):\n",
    "    pos_sim, neg_sim = mat_dist(input_labels, inputs)\n",
    "    ap = K.clip(- K.tf.stop_gradient(pos_sim) + 1 + margin, min_value=0., max_value=2.)\n",
    "    an = K.clip(K.tf.stop_gradient(neg_sim) + margin, min_value=0., max_value=2.)\n",
    "    \n",
    "    delta_p = 1 - margin\n",
    "    delta_n = margin\n",
    "\n",
    "    logit_p = - ap * (pos_sim - delta_p) * gamma\n",
    "    logit_n = an * (neg_sim - delta_n) * gamma\n",
    "    loss = K.softplus(K.logsumexp(logit_p)+K.logsumexp(logit_n))\n",
    "\n",
    "    return loss# K.max(an), K.min(an)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 237.5916 - val_loss: 237.5716\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 223.8533 - val_loss: 231.6416\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 202.6574 - val_loss: 231.0028\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 196.5675 - val_loss: 230.8021\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 192.7726 - val_loss: 229.0677\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 190.4645 - val_loss: 228.5386\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 188.4791 - val_loss: 226.6956\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 186.9846 - val_loss: 229.5151\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 185.6329 - val_loss: 225.4161\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 184.7511 - val_loss: 227.1946\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 183.7450 - val_loss: 226.2246\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 182.7629 - val_loss: 226.1476\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 182.4122 - val_loss: 225.9746\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 181.9564 - val_loss: 226.4970\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 180.8014 - val_loss: 223.7173\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 180.6071 - val_loss: 225.0145\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 180.4815 - val_loss: 224.1794\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 179.9633 - val_loss: 223.9899\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 179.4326 - val_loss: 223.7595\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 178.4858 - val_loss: 222.0014\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 178.4101 - val_loss: 221.7862\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 178.1437 - val_loss: 223.0990\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 177.0980 - val_loss: 222.8854\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 177.3332 - val_loss: 220.6596\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 176.5102 - val_loss: 219.5357\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 176.5787 - val_loss: 220.4093\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 175.6102 - val_loss: 219.6508\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 175.0648 - val_loss: 220.9277\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 174.8886 - val_loss: 219.2849\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 174.3786 - val_loss: 218.6890\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 174.2365 - val_loss: 216.1635\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 174.2408 - val_loss: 216.4549\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 173.1195 - val_loss: 218.2052\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 173.0628 - val_loss: 216.7172\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 173.0062 - val_loss: 214.9791\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 172.8649 - val_loss: 218.5988\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 173.1365 - val_loss: 216.8855\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 172.0686 - val_loss: 214.8015\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 171.9935 - val_loss: 214.2169\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 171.5730 - val_loss: 213.4229\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 171.2341 - val_loss: 214.3337\n",
      "Epoch 42/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 171.3945 - val_loss: 210.1488\n",
      "Epoch 43/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 170.3705 - val_loss: 210.6119\n",
      "Epoch 44/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 170.2758 - val_loss: 215.1776\n",
      "Epoch 45/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 170.4098 - val_loss: 213.8061\n",
      "Epoch 46/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 169.8191 - val_loss: 210.2219\n",
      "Epoch 47/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 169.5900 - val_loss: 213.0062\n",
      "Epoch 48/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 168.9699 - val_loss: 210.2806\n",
      "Epoch 49/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 169.5913 - val_loss: 212.3864\n",
      "Epoch 50/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 168.5615 - val_loss: 211.9002\n",
      "Epoch 51/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 168.9086 - val_loss: 211.1789\n",
      "Epoch 52/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 168.6108 - val_loss: 212.1899\n",
      "Epoch 53/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 168.0836 - val_loss: 210.0214\n",
      "Epoch 54/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 168.1967 - val_loss: 211.2577\n",
      "Epoch 55/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 168.5778 - val_loss: 210.2070\n",
      "Epoch 56/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 168.0988 - val_loss: 206.7210\n",
      "Epoch 57/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 167.6562 - val_loss: 207.3210\n",
      "Epoch 58/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 167.7898 - val_loss: 206.7862\n",
      "Epoch 59/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 167.3383 - val_loss: 207.1222\n",
      "Epoch 60/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 166.7875 - val_loss: 207.8775\n",
      "Epoch 61/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 167.0595 - val_loss: 205.3186\n",
      "Epoch 62/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 166.6211 - val_loss: 206.7578\n",
      "Epoch 63/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 166.9332 - val_loss: 206.7576\n",
      "Epoch 64/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 166.6497 - val_loss: 206.5836\n",
      "Epoch 65/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 166.7283 - val_loss: 209.9681\n",
      "Epoch 66/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 166.4238 - val_loss: 205.5748\n",
      "Epoch 67/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 166.1112 - val_loss: 204.4496\n",
      "Epoch 68/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 165.8673 - val_loss: 206.3920\n",
      "Epoch 69/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 166.0524 - val_loss: 209.2447\n",
      "Epoch 70/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 166.1487 - val_loss: 206.0645\n",
      "Epoch 71/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 165.9784 - val_loss: 206.4658\n",
      "Epoch 72/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 165.6102 - val_loss: 203.6486\n",
      "Epoch 73/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 165.5367 - val_loss: 205.9316\n",
      "Epoch 74/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 165.5221 - val_loss: 202.7538\n",
      "Epoch 75/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 165.6225 - val_loss: 204.7371\n",
      "Epoch 76/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 165.1254 - val_loss: 202.6090\n",
      "Epoch 77/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 165.8171 - val_loss: 203.9924\n",
      "Epoch 78/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 164.5053 - val_loss: 203.9456\n",
      "Epoch 79/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 164.9263 - val_loss: 203.8541\n",
      "Epoch 80/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 164.2145 - val_loss: 204.1794\n",
      "Epoch 81/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 164.1316 - val_loss: 202.3930\n",
      "Epoch 82/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 164.4096 - val_loss: 204.3112\n",
      "Epoch 83/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 164.5003 - val_loss: 205.9398\n",
      "Epoch 84/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 164.4421 - val_loss: 203.1044\n",
      "Epoch 85/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 163.6066 - val_loss: 199.7532\n",
      "Epoch 86/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 164.2069 - val_loss: 200.7232\n",
      "Epoch 87/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 163.9487 - val_loss: 200.8222\n",
      "Epoch 88/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 163.9034 - val_loss: 198.3919\n",
      "Epoch 89/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 163.3482 - val_loss: 199.5803\n",
      "Epoch 90/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 163.6177 - val_loss: 199.5871\n",
      "Epoch 91/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 163.0841 - val_loss: 202.0495\n",
      "Epoch 92/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 162.8556 - val_loss: 198.6846\n",
      "Epoch 93/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 163.0698 - val_loss: 199.5526\n",
      "Epoch 94/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 162.7354 - val_loss: 200.2141\n",
      "Epoch 95/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 163.0973 - val_loss: 199.7277\n",
      "Epoch 96/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 162.8820 - val_loss: 197.9757\n",
      "Epoch 97/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 162.6159 - val_loss: 199.4179\n",
      "Epoch 98/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 162.2842 - val_loss: 196.2836\n",
      "Epoch 99/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 161.8848 - val_loss: 200.2550\n",
      "Epoch 100/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 162.4496 - val_loss: 196.9825\n",
      "Epoch 101/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 162.2158 - val_loss: 200.8295\n",
      "Epoch 102/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 161.9812 - val_loss: 196.6251\n",
      "Epoch 103/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 161.9810 - val_loss: 193.6818\n",
      "Epoch 104/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 161.3523 - val_loss: 198.7080\n",
      "Epoch 105/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 161.4849 - val_loss: 197.8263\n",
      "Epoch 106/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 161.7182 - val_loss: 193.5334\n",
      "Epoch 107/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 161.7485 - val_loss: 195.0230\n",
      "Epoch 108/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 161.1121 - val_loss: 195.1554\n",
      "Epoch 109/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 161.8345 - val_loss: 196.5167\n",
      "Epoch 110/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 161.5722 - val_loss: 193.6966\n",
      "Epoch 111/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 161.5171 - val_loss: 197.2474\n",
      "Epoch 112/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 160.8068 - val_loss: 194.7282\n",
      "Epoch 113/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 161.8127 - val_loss: 193.2513\n",
      "Epoch 114/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 160.4501 - val_loss: 194.3924\n",
      "Epoch 115/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 161.0563 - val_loss: 190.1156\n",
      "Epoch 116/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 160.7919 - val_loss: 191.1259\n",
      "Epoch 117/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 160.5738 - val_loss: 192.6022\n",
      "Epoch 118/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 160.9504 - val_loss: 192.9096\n",
      "Epoch 119/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 160.5962 - val_loss: 194.3062\n",
      "Epoch 120/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 160.5659 - val_loss: 191.3681\n",
      "Epoch 121/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 160.3265 - val_loss: 190.5478\n",
      "Epoch 122/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 160.0536 - val_loss: 190.4425\n",
      "Epoch 123/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 160.5640 - val_loss: 189.7343\n",
      "Epoch 124/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 160.7664 - val_loss: 191.7629\n",
      "Epoch 125/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 159.5970 - val_loss: 188.7746\n",
      "Epoch 126/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 159.7547 - val_loss: 192.0338\n",
      "Epoch 127/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 160.4250 - val_loss: 187.6113\n",
      "Epoch 128/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 159.7771 - val_loss: 186.5674\n",
      "Epoch 129/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 159.7212 - val_loss: 191.6650\n",
      "Epoch 130/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 159.4970 - val_loss: 190.5281\n",
      "Epoch 131/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 158.9239 - val_loss: 187.3677\n",
      "Epoch 132/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 159.8019 - val_loss: 188.6384\n",
      "Epoch 133/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 159.4184 - val_loss: 188.8845\n",
      "Epoch 134/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 158.7788 - val_loss: 187.4663\n",
      "Epoch 135/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 159.2035 - val_loss: 187.2824\n",
      "Epoch 136/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 158.7071 - val_loss: 188.2045\n",
      "Epoch 137/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 158.9247 - val_loss: 188.7264\n",
      "Epoch 138/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 159.0161 - val_loss: 190.2162\n",
      "Epoch 139/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 159.3102 - val_loss: 188.1389\n",
      "Epoch 140/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 158.7223 - val_loss: 184.4470\n",
      "Epoch 141/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 158.2674 - val_loss: 187.1081\n",
      "Epoch 142/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 158.5089 - val_loss: 186.5565\n",
      "Epoch 143/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 158.2706 - val_loss: 185.4593\n",
      "Epoch 144/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 157.9084 - val_loss: 183.2535\n",
      "Epoch 145/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 157.3545 - val_loss: 182.6628\n",
      "Epoch 146/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 158.2635 - val_loss: 185.1308\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 66us/step - loss: 158.1205 - val_loss: 189.5111\n",
      "Epoch 148/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 158.7630 - val_loss: 190.2211\n",
      "Epoch 149/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 157.8352 - val_loss: 184.9723\n",
      "Epoch 150/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 157.9588 - val_loss: 189.2230\n",
      "Epoch 151/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 157.5085 - val_loss: 183.7452\n",
      "Epoch 152/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 157.7968 - val_loss: 182.9408\n",
      "Epoch 153/200\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 157.6236 - val_loss: 184.2920\n",
      "Epoch 154/200\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 157.3975 - val_loss: 181.5109\n",
      "Epoch 155/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 157.0771 - val_loss: 181.9331\n",
      "Epoch 156/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 157.2575 - val_loss: 181.4021\n",
      "Epoch 157/200\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 157.3845 - val_loss: 186.0808\n",
      "Epoch 158/200\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 157.4730 - val_loss: 185.2588\n",
      "Epoch 159/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 156.7880 - val_loss: 179.0817\n",
      "Epoch 160/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 156.6942 - val_loss: 186.4224\n",
      "Epoch 161/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 156.9441 - val_loss: 180.0899\n",
      "Epoch 162/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 157.0686 - val_loss: 182.8661\n",
      "Epoch 163/200\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 156.1016 - val_loss: 177.7202\n",
      "Epoch 164/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 156.0723 - val_loss: 181.3436\n",
      "Epoch 165/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 156.0856 - val_loss: 179.6926\n",
      "Epoch 166/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 156.0184 - val_loss: 180.3327\n",
      "Epoch 167/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 156.4561 - val_loss: 180.2968\n",
      "Epoch 168/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 155.7081 - val_loss: 180.5984\n",
      "Epoch 169/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 156.5453 - val_loss: 182.1160\n",
      "Epoch 170/200\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 155.9616 - val_loss: 178.7159\n",
      "Epoch 171/200\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 155.9227 - val_loss: 180.1721\n",
      "Epoch 172/200\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 156.0050 - val_loss: 178.5359\n",
      "Epoch 173/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 155.6179 - val_loss: 179.6507\n",
      "Epoch 174/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 155.0544 - val_loss: 178.1836\n",
      "Epoch 175/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 156.2251 - val_loss: 178.3928\n",
      "Epoch 176/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 155.4277 - val_loss: 177.1944\n",
      "Epoch 177/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 155.5224 - val_loss: 179.3277\n",
      "Epoch 178/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 155.1562 - val_loss: 176.6730\n",
      "Epoch 179/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 155.3500 - val_loss: 179.7422\n",
      "Epoch 180/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 155.4909 - val_loss: 176.1929\n",
      "Epoch 181/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 154.8385 - val_loss: 175.5371\n",
      "Epoch 182/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 154.7275 - val_loss: 177.2439\n",
      "Epoch 183/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 154.9075 - val_loss: 178.2319\n",
      "Epoch 184/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 154.5075 - val_loss: 176.4636\n",
      "Epoch 185/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 155.3776 - val_loss: 178.3807\n",
      "Epoch 186/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 154.5216 - val_loss: 176.2985\n",
      "Epoch 187/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 154.2881 - val_loss: 175.1009\n",
      "Epoch 188/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 154.3046 - val_loss: 174.3665\n",
      "Epoch 189/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 154.6466 - val_loss: 172.9987\n",
      "Epoch 190/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 154.4669 - val_loss: 174.5187\n",
      "Epoch 191/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 154.1148 - val_loss: 174.4094\n",
      "Epoch 192/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 154.7962 - val_loss: 176.6113\n",
      "Epoch 193/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 154.3484 - val_loss: 173.5906\n",
      "Epoch 194/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 153.8648 - val_loss: 174.6171\n",
      "Epoch 195/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 153.1363 - val_loss: 174.6874\n",
      "Epoch 196/200\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 153.7365 - val_loss: 172.8160\n",
      "Epoch 197/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 154.1751 - val_loss: 173.7855\n",
      "Epoch 198/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 153.6329 - val_loss: 174.0618\n",
      "Epoch 199/200\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 153.5496 - val_loss: 175.5508\n",
      "Epoch 200/200\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 153.7919 - val_loss: 177.0642\n",
      "Test loss: 153.8179127441406\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=circle_loss,\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=[])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score)\n",
    "# print('Test accuracy:', score[1])\n",
    "\n",
    "## with_model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 2.2707 - acc: 0.2390 - val_loss: 2.2212 - val_acc: 0.3929\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 2.1754 - acc: 0.5037 - val_loss: 2.1293 - val_acc: 0.6168\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 2.0856 - acc: 0.7638 - val_loss: 2.0414 - val_acc: 0.8782\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 1.9993 - acc: 0.9280 - val_loss: 1.9567 - val_acc: 0.9519\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 1.9161 - acc: 0.9694 - val_loss: 1.8751 - val_acc: 0.9717\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 1.8359 - acc: 0.9827 - val_loss: 1.7964 - val_acc: 0.9790\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 1.7587 - acc: 0.9881 - val_loss: 1.7208 - val_acc: 0.9830\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 1.6846 - acc: 0.9904 - val_loss: 1.6483 - val_acc: 0.9850\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 1.6135 - acc: 0.9921 - val_loss: 1.5789 - val_acc: 0.9858\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 1.5455 - acc: 0.9930 - val_loss: 1.5124 - val_acc: 0.9868\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 1.4805 - acc: 0.9938 - val_loss: 1.4490 - val_acc: 0.9872\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 1.4184 - acc: 0.9942 - val_loss: 1.3885 - val_acc: 0.9872\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 1.3593 - acc: 0.9945 - val_loss: 1.3309 - val_acc: 0.9876\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 1.3030 - acc: 0.9948 - val_loss: 1.2761 - val_acc: 0.9880\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 1.2495 - acc: 0.9951 - val_loss: 1.2240 - val_acc: 0.9881\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 1.1986 - acc: 0.9953 - val_loss: 1.1746 - val_acc: 0.9883\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 1.1504 - acc: 0.9955 - val_loss: 1.1278 - val_acc: 0.9884\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 1.1047 - acc: 0.9956 - val_loss: 1.0834 - val_acc: 0.9888\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 1.0614 - acc: 0.9956 - val_loss: 1.0414 - val_acc: 0.9891\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 1.0204 - acc: 0.9957 - val_loss: 1.0016 - val_acc: 0.9891\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.9816 - acc: 0.9959 - val_loss: 0.9640 - val_acc: 0.9893\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.9448 - acc: 0.9959 - val_loss: 0.9284 - val_acc: 0.9893\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.9101 - acc: 0.9960 - val_loss: 0.8947 - val_acc: 0.9894\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.8772 - acc: 0.9960 - val_loss: 0.8628 - val_acc: 0.9895\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.8461 - acc: 0.9961 - val_loss: 0.8327 - val_acc: 0.9895\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.8167 - acc: 0.9961 - val_loss: 0.8042 - val_acc: 0.9895\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.7888 - acc: 0.9961 - val_loss: 0.7772 - val_acc: 0.9896\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.7624 - acc: 0.9962 - val_loss: 0.7516 - val_acc: 0.9896\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.7375 - acc: 0.9961 - val_loss: 0.7274 - val_acc: 0.9896\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.7138 - acc: 0.9961 - val_loss: 0.7045 - val_acc: 0.9896\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.6914 - acc: 0.9961 - val_loss: 0.6828 - val_acc: 0.9896\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.6701 - acc: 0.9961 - val_loss: 0.6622 - val_acc: 0.9896\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.6499 - acc: 0.9961 - val_loss: 0.6427 - val_acc: 0.9896\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.6308 - acc: 0.9962 - val_loss: 0.6241 - val_acc: 0.9896\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.6126 - acc: 0.9961 - val_loss: 0.6065 - val_acc: 0.9895\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.5953 - acc: 0.9961 - val_loss: 0.5897 - val_acc: 0.9895\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.5789 - acc: 0.9961 - val_loss: 0.5738 - val_acc: 0.9895\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.5632 - acc: 0.9962 - val_loss: 0.5587 - val_acc: 0.9894\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.5483 - acc: 0.9962 - val_loss: 0.5442 - val_acc: 0.9894\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.5341 - acc: 0.9962 - val_loss: 0.5304 - val_acc: 0.9893\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.5206 - acc: 0.9962 - val_loss: 0.5173 - val_acc: 0.9893\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.5077 - acc: 0.9962 - val_loss: 0.5048 - val_acc: 0.9894\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4954 - acc: 0.9962 - val_loss: 0.4928 - val_acc: 0.9894\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4836 - acc: 0.9962 - val_loss: 0.4814 - val_acc: 0.9894\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4723 - acc: 0.9963 - val_loss: 0.4705 - val_acc: 0.9894\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4615 - acc: 0.9962 - val_loss: 0.4600 - val_acc: 0.9894\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4512 - acc: 0.9963 - val_loss: 0.4500 - val_acc: 0.9894\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4413 - acc: 0.9963 - val_loss: 0.4404 - val_acc: 0.9894\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4319 - acc: 0.9962 - val_loss: 0.4312 - val_acc: 0.9894\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4228 - acc: 0.9962 - val_loss: 0.4224 - val_acc: 0.9894\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4141 - acc: 0.9962 - val_loss: 0.4140 - val_acc: 0.9895\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4057 - acc: 0.9962 - val_loss: 0.4058 - val_acc: 0.9897\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3977 - acc: 0.9962 - val_loss: 0.3980 - val_acc: 0.9897\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3899 - acc: 0.9962 - val_loss: 0.3905 - val_acc: 0.9897\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3825 - acc: 0.9962 - val_loss: 0.3833 - val_acc: 0.9897\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3753 - acc: 0.9962 - val_loss: 0.3763 - val_acc: 0.9897\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3684 - acc: 0.9962 - val_loss: 0.3696 - val_acc: 0.9897\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3618 - acc: 0.9962 - val_loss: 0.3632 - val_acc: 0.9896\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3554 - acc: 0.9962 - val_loss: 0.3569 - val_acc: 0.9895\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3492 - acc: 0.9962 - val_loss: 0.3509 - val_acc: 0.9895\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3432 - acc: 0.9962 - val_loss: 0.3451 - val_acc: 0.9895\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3375 - acc: 0.9962 - val_loss: 0.3395 - val_acc: 0.9895\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3319 - acc: 0.9962 - val_loss: 0.3341 - val_acc: 0.9896\n",
      "Epoch 64/100\n",
      "  128/60000 [..............................] - ETA: 1s - loss: 0.3505 - acc: 0.9922"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8202874a1859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m           validation_data=(tmp_test, y_test))\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/data/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_model = Sequential()\n",
    "clf_model.add(Dense(num_classes, activation='softmax'))\n",
    "clf_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# x_batch, y_batch = model.predict(x_train[:batch_size]), keras.utils.to_categorical(y_train[:batch_size], num_classes)\n",
    "# x_batch, y_batch = model.predict(x_train[:batch_size]), y_train[:batch_size]\n",
    "# print(x_batch.shape, y_batch.shape)\n",
    "# clf_model.train_on_batch(x_batch, y_batch)\n",
    "\n",
    "tmp_train = model.predict(x_train)\n",
    "tmp_train = tmp_train / np.linalg.norm(tmp_train, axis=1)[:, np.newaxis]\n",
    "tmp_test = model.predict(x_test)\n",
    "tmp_test = tmp_test / np.linalg.norm(tmp_test, axis=1)[:, np.newaxis]\n",
    "\n",
    "clf_model.fit(tmp_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(tmp_test, y_test))\n",
    "score = clf_model.evaluate(tmp_test, y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "mlp.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "mlp.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "mlp.add(Dropout(rate=0.25))\n",
    "mlp.add(Flatten())\n",
    "mlp.add(Dense(128, activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(num_classes, activation='softmax'))\n",
    "mlp.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "mlp.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = mlp.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without_model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 9891\n"
     ]
    }
   ],
   "source": [
    "stereo_type = []\n",
    "for i in range(num_classes):\n",
    "    tmp_i = model.predict(x_train[y_train==i])\n",
    "    tmp_i = tmp_i / np.linalg.norm(tmp_i, axis=1)[:, np.newaxis]\n",
    "    tmp_i = np.mean(tmp_i, axis=0)\n",
    "    stereo_type.append(tmp_i)           \n",
    "stereo_type = np.array(stereo_type)\n",
    "\n",
    "tmp_test = model.predict(x_test)\n",
    "tmp_test = tmp_test / np.linalg.norm(tmp_test, axis=1)[:, np.newaxis]\n",
    "label = np.argmax(np.matmul(tmp_test, stereo_type.T), axis=1)\n",
    "print(\"acc\", np.sum(y_test == label))\n",
    "# stereo_type = [np.mean(model.predict(x_train[y_train==i]), axis=0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8886915 0.65426034\n"
     ]
    }
   ],
   "source": [
    "pos_sim, neg_sim = sess.run(mat_dist(y_train[:batch_size], model.predict(x_train[:batch_size])))\n",
    "print(np.mean(pos_sim), np.mean(neg_sim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}